{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c996cd69",
   "metadata": {},
   "source": [
    "# Discriminative Classification\n",
    "\n",
    "**Generative Classification**: mappo TUTTA la pdf del feature space dei miei dati, e in base a quanto ho appena fatto, determino \"quanto sia probabile che il nuovo elemento X\" appartenga alle varie popolazioni.\n",
    "\n",
    "**Discriminative Classification**: a 'sto giro mi interessa *solo* il decision boundary, NON la probabilità. -> *problema*: what is the most optimal way to find the boundary, say, between two blobs of data? E che succede se i due blob non sono perfettamente separati?\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "Uso un modello ~ identico a quanto usato in LDA, con l'unica differenza nel calcolo dei regression coefficients. In questo caso, voglio minimizzare il classification error. In LDA, voglio minimizzare il density estimation error.\n",
    "\n",
    "### SVM: Support Vector Machines\n",
    "\n",
    "Le SVM definiscono un iperpiano (N-1 dimensioni) che massimizza la distanza del punto più vicino da ciascuna classe. Questa distanza si chiama \"margine\" (vedi immagini) -> I punti che \"toccano\" il margine si chiamano **support vector**. Siccome ci sono molti potenziali decision boundaries, voglio prendere quello che MASSIMIZZA la distanza del support vector dal decision hyperplane.\n",
    "\n",
    "Dove il decision boundary è ovvio, posso assumere che le classi siano LINEARMENTE separabili; per i data set realistici, devo rilassare questa ipotesi.\n",
    "\n",
    "Guarda il notebook per vedere come funziona `sklearn.SVC`. Il parametro $C$ è un *regularization parameter*, strettamente positivo, e la \"forza\" della regolarizzazione è inversamente proporzionale a $C$. **È l'inverso della $\\alpha$ della regression.** In altre parole, controlla quante \"violazioni\" posso avere del margine, e quindi la width del mio \"ipercilindro\" tra i dati.\n",
    "\n",
    "Nota che SVM dipende solo dai support vector, quindi puoi BUTTARE il resto dei dati.\n",
    "\n",
    "Guarda l'esempio per esteso sul suo notebook. Commenti:\n",
    "\n",
    "- La mediana di una distribuzione non è alterata da grandi perturbazioni di outliers, finché queste perturbazioni NON superano il bordo (?)\n",
    "- Massimizzare il margine dei support vector al posto di usare tutti i data points rende la SVM classification molto simile ai rank-based estimators -> **big pro**: quando hai determinato i SV, a quel punto cambiare posizione o numero dei punti oltre al margine NON cambia il decision boundary!\n",
    "- **big con** -> high completeness in my data, at the cost of large contamination levels\n",
    "\n",
    "Nota bene:\n",
    "\n",
    "+ SVM is NOT scale invariant\n",
    "+ The data do NOT need to be separable.\n",
    "\n",
    "### Kernel Methods\n",
    "\n",
    "Se la contaminazione è guidata da NON-linear effects, potresti cercare un non-linear decision boundary -> **kernelization**.\n",
    "\n",
    "Ci sono diversi esempi sul notebook, e sono molto carini.\n",
    "\n",
    "### Decision Trees\n",
    "\n",
    "Simile ad un algoritmo che farei \"a mano\":\n",
    "\n",
    "1. definisci criteri per separare i dati in due gruppi\n",
    "2. ripeti sui sottogruppi\n",
    "3. ripeti finché non trovi uno stopping point, es. hai un numero minimo di item da splittare\n",
    "\n",
    "Funzionamento: top node, ogni branch ha two child nodes; lo split si basa su PREDEFINED decision boundary, e lo splitting si ripete RICORSIVAMENTE finché raggiungo PREDEFINED stopping criteria.\n",
    "\n",
    "Esempio sul notebook: classificazione animali. I **leaf notes** registrano la % di punti che sono appartenenti ad una o l'altra classe nel training set.\n",
    "\n",
    "*Binary Splitting: SIMPLE. Asking the RIGHT questions: HARD*\n",
    "\n",
    "##### Splitting Criteria\n",
    "\n",
    "1. Information Content, E(x), dei dati\n",
    "\n",
    "Definisco *Information Gain* la riduzione di entropia dovuta al partizionare in quel modo i miei dati. La cosiddetta IG viene anche chiamata *Kullback Leibler (KL) Divergence*.\n",
    "\n",
    "Fai *trial splits* along each feature, one at a time; la feature che dà massima IG è quella con cui devi fare lo split.\n",
    "\n",
    "2. Gini Coefficient\n",
    "\n",
    "Praticamente, sto stimando la probabilità di classificazione INCORRETTA scegliendo sia un punto e, separatamente, una classe casuale dai miei dati.\n",
    "\n",
    "> SE I TUOI CRITERI SONO \"LOOSE\", RISCHI SOLTANTO DI CREARE NOISE SPLITTANDO TROPPO! -> Usa Cross-Validation per ottimizzare la depth of the tree ed evitare overfitting ( stai facendo regularization, praticamente )\n",
    "\n",
    "**A differenza degli SVM, i decision trees sono molto instabili in caso di cambiamenti nei dati!**\n",
    "\n",
    "### Ensemble Learning\n",
    "\n",
    "Posso fare un qualche \"weighted voting\"? -> Ensemble Learning\n",
    "\n",
    "#### Bagging (= Bootstrap Aggregation)\n",
    "\n",
    "Posso migliorare parecchio la performance dei decision trees: il bagging media i risultati predittivi di una serie di BOOTSTRAP SAMPLES dei dati originali.\n",
    "\n",
    "Guarda l'esempio nel notebook.\n",
    "\n",
    "##### Random Forests\n",
    "\n",
    "Estendo il Bagging generando decision trees dai bootstrap samples\n",
    "\n",
    "* Le splitting features con cui crei i tuoi tree sono SELEZIONATE A CASO tra il FULL set of features dei dati\n",
    "* Il numero di features selezionate per split level è tipicamente la radice del numero totale delle feature\n",
    "* La final classification è fatta MEDIANDO le classificazioni di ciascun singolo decision tree.\n",
    "* Supero alcune limitazioni dei decision trees, e posso parallelizzare i processi -> FAST\n",
    "\n",
    "Si usa di nuovo la cross-validation per determinare l'optimal depth\n",
    "\n",
    "Esempio disponibile sul notebook.\n",
    "\n",
    "#### Boosting\n",
    "\n",
    "Ensemble approach dove MANY WEAK CLASSIFIERS are COMBINED and IMPROVED UPON to make classification better. **Il boosting crea dei modelli che cercano di correggere gli errori del mio ensemble, so far.**\n",
    "\n",
    "> Al cuore del boosting sta l'idea che ri-pesiamo i dati, in base a quanto sono stati classificati ERRONEAMENTE all'iterazione precedente. Questo concetto è FONDAMENTALE in deep learning!\n",
    "\n",
    "La forma più popolare di boosting si chiama *adaptive boosting*.\n",
    "\n",
    "**Fundamental Limitation:** il computation time. A differenza del random forest, NON posso mettere i processi in parallelo .-.\n",
    "\n",
    "Esempio sul notebook.\n",
    "\n",
    "### OK, BUT WHICH ONE SHOULD I USE?\n",
    "\n",
    "No single model can be known in advance to be the best classifier! Vedi Ivezic, table 9.1\n",
    "\n",
    "Vedi ultimo esempio sul notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
