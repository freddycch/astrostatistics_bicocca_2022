{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability: Central Limit Theorem\n",
    "\n",
    "MC method: sommo cose. Però perché la mia distribuzione nell'esercizio 1 della lezione 3 dopo diventa una curva gaussiana o ci assomiglia?\n",
    "\n",
    "It all has to do with the fact that I'm summing stuff -> Central Limit Theorem\n",
    "\n",
    "#### Central Limit Theorem\n",
    "\n",
    "> For an arbitrary distribution, $h(x)$, with a well-defined mean, $\\mu$, and standard deviation, $\\sigma$ (i.e. tails should fall off faster than $1/x^2$) the mean of $N$ values \\{$x_i$\\} drawn from the distribution will follow a Gaussian Distribution with $\\mathcal{N}(\\mu,\\sigma/\\sqrt{N})$. (A Cauchy distribution is one example where this fails.)\n",
    "\n",
    "Estremamente potente. PERCHE' una distribuzione di numeri, che arriva da QUALSIASI cosa, tende a una gaussiana?\n",
    "\n",
    "Lo spread scala con la sqrt(N), tra l'altro. ( N numero di tentativi )\n",
    "\n",
    "#### Bivariate and Multivariate p.d.f.\n",
    "\n",
    "Up to now we have been dealing with one-dimensional distribution functions.  Let's now consider a two dimensional distribution $h(x,y)$ where $$\\int_{-\\infty}^{\\infty}dx\\int_{-\\infty}^{\\infty}h(x,y)dy = 1.$$  $h(x,y)$ is telling us the probability that $x$ is between $x$ and $dx$ and *also* that $y$ is between $y$ and $dy$.\n",
    "\n",
    "Then we have the following definitions:\n",
    "\n",
    "$$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma^2_y = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(y-\\mu_y)^2 h(x,y) dx dy$$\n",
    "\n",
    "$$\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$$\n",
    "\n",
    "$$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "If $x$ and $y$ are uncorrelated, then we can treat the system as two independent 1-D distributions.  This means that choosing a range on one variable has no effect on the distribution of the other.\n",
    "\n",
    "We can write a 2-D Gaussian pdf as\n",
    "$$p(x,y|\\mu_x,\\mu_y,\\sigma_x,\\sigma_y,\\sigma_{xy}) = \\frac{1}{2\\pi \\sigma_x \\sigma_y \\sqrt{1-\\rho^2}} \\exp\\left(\\frac{-z^2}{2(1-\\rho^2)}\\right),$$\n",
    "\n",
    "where $$z^2 = \\frac{(x-\\mu_x)^2}{\\sigma_x^2} + \\frac{(y-\\mu_y)^2}{\\sigma_y^2} - 2\\rho\\frac{(x-\\mu_x)(y-\\mu_y)}{\\sigma_x\\sigma_y},$$\n",
    "\n",
    "with $$\\rho = \\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}$$\n",
    "as the **(dimensionless) correlation coefficient**.\n",
    "\n",
    "If $x$ and $y$ are perfectly correlated then $\\rho=\\pm1$ and if they are uncorrelated, then $\\rho=0$.\n",
    "\n",
    "La distribuzione ovviamente diventa un istogramma 3D o un countour plot, a seconda di come la vuoi disegnare.\n",
    "\n",
    "Vedi immagine: isocountour, mi chiedo chi sia l'inclinazione $\\alpha$. Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Riesco a cambiare il modo in cui definisco le variabili e \"seccare\" la correlation? ( Alias, giro $\\alpha$ finché le variabili appaiono non correlate )\n",
    "\n",
    "We can define new coordinate axes that are aligned with the minimum and maximum widths of the distribution. (Or in different terminology, the semi-minor and semi-major axes of the ellipse). These are called the **principal axes** and are given by\n",
    "\n",
    "$$P_1 = (x-\\mu_x)\\cos\\alpha + (y-\\mu_y)\\sin\\alpha,$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P_2 = -(x-\\mu_x)\\sin\\alpha + (y-\\mu_y)\\cos\\alpha.$$\n",
    "\n",
    "The widths in this (rotated) coordinate system are\n",
    "\n",
    "$$\\sigma^2_{1,2} = \\frac{\\sigma_x^2+\\sigma_y^2}{2}\\pm\\sqrt{\\left(\\frac{\\sigma_x^2-\\sigma_y^2}{2}\\right)^2 + \\sigma^2_{xy}}.$$\n",
    "\n",
    "Note that **the correlation vanishes in this coordinate system (by definition)** and the bivariate Gaussian is just a product of two univariate Gaussians.  \n",
    "\n",
    "This concept will be crucial for understanding ***Principal Component Analysis*** when we get to Chapter 7, where PCA extends this idea to even more dimensions.\n",
    "\n",
    "##### In altre parole stai facendo UNSUPERVISED CLUSTERING (?)\n",
    "\n",
    "La mediana è più utile della media anche in più dimensioni.\n",
    "\n",
    "#### Dimensione Arbitraria\n",
    "\n",
    "We can generalize the way we describe **Gaussian distributions in multiple dimensions**, $M$, through the elegance of linear algebra. Instead of writing everything in terms of separate coordinates, we can bundle everything together in an $M$-dimensional coordinate vector $\\vec{x}$, mean vector $\\vec{\\mu}$, and covariance matrix $\\mathbf{C} = E([\\vec{x}-\\vec{\\mu}][\\vec{x}-\\vec{\\mu}]^T)$.\n",
    "\n",
    "$$p(\\vec{x}|\\vec{\\mu},\\mathbf{C}) = \\frac{1}{\\sqrt{\\mathrm{det}(2\\pi\\mathbf{C})}} \\exp\\left[-\\frac{1}{2}(\\vec{x}-\\vec{\\mu})^T \\mathbf{C}^{-1} (\\vec{x}-\\vec{\\mu}) \\right] $$\n",
    "\n",
    "where \n",
    "\n",
    "$$ C_{kj} = \\int_{-\\infty}^\\infty (x^k-\\mu^k)(x^j-\\mu^j)p(\\vec{x}|\\vec{\\mu},\\mathbf{C})\\,d^M x $$\n",
    "\n",
    "and \n",
    "\n",
    "$$ (\\vec{x}-\\vec{\\mu})^T \\mathbf{C}^{-1} (\\vec{x}-\\vec{\\mu}) = \\sum_{k=1}^M \\sum_{j=1}^M [\\mathbf{C}^{-1}]_{kj}(x^k-\\mu^k)(x^j-\\mu^j) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confidence Intervals\n",
    "\n",
    "Di solito faccio il lavoro coi percentili ma SOLO in UNA dimensione. Quando aumento le dimensioni, diventa più problematico! NON FUNZIONA NELLO STESSO MODO!\n",
    "\n",
    "In two dimensions, a Gaussian density can be described by\n",
    "\n",
    "$$ \\mathrm{pdf}(r) = \\frac{1}{2\\pi s^2}\\exp\\left[-\\frac{1}{2}\\left(\\frac{r}{s}\\right)^2\\right] $$\n",
    "\n",
    "with the cdf (using polar coordinates and implicitly integrating out the angle)\n",
    "\n",
    "$$ \\mathrm{cdf}(x) = 1 - \\exp(-(x/s)^2/2) $$\n",
    "\n",
    "This means that within \"1-sigma\", the Gaussian contains 1-exp(-0.5) $\\sim 0.393$ or $39.3\\%$ of the volume. Therefore in `corner`, the relevant 1-sigma levels for a 2D histogram of samples is 39% not 68%. \n",
    "\n",
    "If you are using `corner` and want the usual 68% of the \"sample mass\" definition of sigma, use the `levels` keyword argument when you call `corner.corner`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corner Plot\n",
    "\n",
    "sto guardando p(x1, x2, x3) e quello nell'immagine è un modo per rappresentarlo.\n",
    "-> faccio marginalized distribution in x1, x2 e x3: sono gli istogrammi nell'img\n",
    "-> faccio marginalized distribution in xi, xj e integro in xz: sono i contour plot 2D che vedi nell'img\n",
    "\n",
    "#### Correlation Coefficients\n",
    "\n",
    "Voglio poter misurare QUANTO CORRELATE sono due variabili.\n",
    "\n",
    "By far, il più famoso coeff. di correlazione è il COEFFICIENTE DI PEARSON: \n",
    "$$ r = \\frac{\\sum_{i=1}^N (x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^N (x_i-\\bar{x})^2}\\sqrt{\\sum_{i=1}^N (y_i-\\bar{y})^2}} $$\n",
    "\n",
    "CAREFUL WITH THE ASSUMPTIONS: se i dati arrivano da gaussiane separate//uncorrelated, la distribuzione di r segue una distribuzione di student. -> puoi usarla come ipotesi nulla \n",
    "\n",
    "    NON incorpora misure sui dati\n",
    "    ESTREMAMENTE sensibile agli outliers\n",
    "\n",
    "Coefficiente di SPEARMAN: $$ r_S = \\frac{\\sum_{i=1}^N (R^x_i-\\bar{R^x})(R^y_i-\\bar{R^y})}{\\sqrt{\\sum_{i=1}^N (R^x_i-\\bar{R^x})^2}\\sqrt{\\sum_{i=1}^N (R^y_i-\\bar{R^y})^2}}. $$\n",
    "\n",
    "cerca di usare RANK di una distribuzion al posto dei suoi valori. E' MOLTO più solido di PEARSON, implementa lui al posto dell'altro.\n",
    "\n",
    "ATTENZIONE: E' INTRINSICALLY BIASED!!!\n",
    "\n",
    "Coefficiente di Kendall: ha a che fare con \"coppie di dati\", definite come \"concordi\" o \"discordi\". Hard pass per ora (imo). Anche lui è BIASED ma molto resistente agli outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DECORATORS\n",
    "\n",
    "@-qualcosa: è un decorator, fa checkpointing strategy. Molto utile per fare delle cose.\n",
    "@pickle_results è fantastica!!!\n",
    "\n",
    "Cerca DECORATOR CHECKPOINTING su internet. Il decoratore di Gerosa arriva da AstroML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from Arbitrary Distributions\n",
    "\n",
    "Solitamente facciamo il sampling da una distribuzione 'speciale', tutto ben implementato.\n",
    "\n",
    "Ma se la distribuzione ha una forma diversa? -> MC methods.\n",
    "\n",
    "Rejection Sampling è meno preferito, ma a volte (2D+) si riduce ad essere l'unico modo.\n",
    "Inverse Sampling è il più facile da implementare in 1D ed è preferito in quel caso.\n",
    "\n",
    "##### Rejection Sampling\n",
    "Lego la distribuzione che non conosco a una distribuzione Q(x) ( PROPOSAL DISTRIBUZION ) che conosco. Deve starci TUTTO il grafico dei miei dati, nb.\n",
    "\n",
    "Genero random numbers BASED ON Q, e una x. Allora TENGO i punti che cadono SOTTO la curva.\n",
    "E' più o meno lo stesso di MC integration. INEFFICIENT: stai BUTTANDO molti punti, e perdi tempo. \n",
    "\n",
    "La generalizzazione in più dimensioni è easy da capire.\n",
    "\n",
    "##### Inverse Transform Sampling\n",
    "\n",
    "in 1d è molto comoda. For any PDF distribution, puoi plottare la CDF e PER DEFINIZIONE lei è definita tra 0 ed 1. Grabbo un numero y UNIFORME tra 0 ed 1, e per definizione trovo la x che mi dà quel valore!\n",
    "\n",
    "Devi prendere la CDF e INVERTIRLA. y = cdf(x) -> x = ... Può essere fattibile in modo analitico, ma a volte non puoi farlo e devi ricorrere a metodi numerici.\n",
    "\n",
    "#### Cloning\n",
    "\n",
    "Prendi pochi dati -> Stimi la pdf -> Sample -> Genero più numeri\n",
    "\n",
    "Questo processo si chiama DISTRIBUTION CLONING."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
