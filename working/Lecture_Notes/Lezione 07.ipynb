{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up da ieri\n",
    "\n",
    "Paragrafo 4.3.1 : goodness of a fit for a model\n",
    "Se il modello è giusto, quanto è probabile che $L^{0}$ venga estratto/arrivi da quel modello?\n",
    "\n",
    "\n",
    "Esercizio di ieri: sono tre gaussiane, l'hai visto. L'AIC viene \"strano\": è un problema del massimizzare/minimizzare le funzioni in più dimensioni! L'AIC si \"incastra\" in un LOCAL maximum: trovo un punto, faccio la derivata, fa come ha spiegato giacomazzo. Ok, però se il massimo che trova l'algoritmo è un massimo locale e non un massimo globale... Auguri: può capitare che succeda una cosa del genere. \n",
    "\n",
    "---\n",
    "\n",
    "## Hypothesis Testing\n",
    "\n",
    "> L'osservazione è in agreement con una certa ipotesi?\n",
    "\n",
    "Frequentist Approach: qual è la probabilità di avere il risultato x?\n",
    "\n",
    "Caso astrofisico: Rejecting the Null Hypothesis != Proving that I have a source\n",
    "\n",
    "GW Strain, frequentist analysis: calcoliamo i p-values ( inverso cdf, in altre parole ) e: \"per ogni data point, qual è la probabilità che il background sia as large as the given data points?\" -> threshold p-values -> costruisco CDF. (?) (?) (?)\n",
    "\n",
    "*Significance*: è un threshold che metti TU, è una cosa soggettiva. Vuoi il Nobel? Magari lo vuoi molto basso, mentre se vuoi semplicemente fare una misura magari ti preoccupi meno.\n",
    "\n",
    "Esempio: flip di una moneta N times.\n",
    "\n",
    "Domanda: ho 14 teste: qual è il p-value per cui posso rigettare la NULL hypothesis che la moneta sia *fair*? -> Mi esce una binomiale, e sommo la coda. ( la .isf option è l'inversa della CDF )\n",
    "\n",
    "### Type I and Type II errors in hypothesis testing\n",
    "\n",
    "1. Ci sono Falsi Positivi \n",
    "2. Ci sono Falsi Negativi\n",
    "\n",
    "Esempio: pixel rotto nel telescopio. Fai la distribuzione come la fa lui, e vedi che...\n",
    "\n",
    "$$ h(x) = (1-a)h_B(x) + ah_S(x)$$\n",
    "\n",
    "where $h_B(x)=\\mathcal{N}(\\mu=100,\\sigma=10)$ is the background distribution (normalized to unity), $h_S(x)=\\mathcal{N}(\\mu=150,\\sigma=12)$ is the source distribution (normalized to unity), and $a$ is the relative normalization factor that accounts for background noise being $(1-a)/a$ more probable than sources.\n",
    "\n",
    "- ***If we set a classificiation threshold at $x_c=120$, calculate the Type I error (false positive) probability, $\\alpha$.***\n",
    "- ***For the same $x_c$, calculate the Type II error (false negative) probability, $\\beta$.***\n",
    "\n",
    "For a sample of size $N$ (containing background noise and sources), the **expected number of spurious sources (Type I / false positives)** is \n",
    "\n",
    "$$ n_\\mathrm{spurious} = N(1-a)\\alpha = N(1-a)\\int_{x_c}^\\infty h_B(x)dx$$ \n",
    "\n",
    "and the **expected number of missed sources (Type II / false negatives)** is\n",
    "\n",
    "$$ n_\\mathrm{missed} = Na\\beta = Na\\int_0^{x_c}h_S(x)dx.$$\n",
    "\n",
    "The **total number of classified sources** (that is number of instances where we reject the null hypothesis) is\n",
    "\n",
    "$$ n_\\mathrm{source} = Na - n_\\mathrm{missed} + n_\\mathrm{spurious} = N[(1-\\beta)a + (1-a)\\alpha].$$\n",
    "\n",
    "The **sample completeness** (or **detection probability**) is defined as\n",
    "\n",
    "$$ \\eta = \\frac{Na - n_\\mathrm{missed}}{Na} = 1-\\int_0^{x_c}h_S(x)dx = 1-\\beta$$\n",
    "\n",
    "Finally, the **sample contamination** is\n",
    "\n",
    "$$ \\epsilon = \\frac{n_\\mathrm{spurious}}{n_\\mathrm{source}}$$\n",
    "\n",
    "where $(1-\\epsilon)$ is sometimes called the **classification efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "**The threshold depends on what you care about**. \n",
    "\n",
    "- Say you're looking for a very rare type of star and you're designing a machine-learning stategy to filter a large of images. Those that are left will need to be analyzed visually (which is painful and time consuming). In this case, you might be willing to accept some contamination, as long and the filtered sample is very complete (better do a bit more manual work than lose the target completely!)\n",
    "\n",
    "- Say you're testing a safety protocol for your computer cluster: you don't want it to explode while you run your very complicated astrophysical simulations. In this case, the contamination needs to be extremely low, even if then the sample is less complete (i.e. you end up discarding many potential builds that would not blow up your cluster after all)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Distributions\n",
    "\n",
    "Strongly related to Hypothesis Testing.\n",
    "\n",
    "Ho due popolazioni da due diversi set di misure: sto re-probing la stessa fisica o no? Qual è la probabilità che arrivino dalla stessa distribuzione by chance? -> p values -> accetto/rigetto la NULL hypothesis in base al mio threshold\n",
    "\n",
    "##### Kolmogorov-Smirnov (KS) Test\n",
    "\n",
    "Cumulative Empirical Distribution: come la CDF, ma *straight out of the data*.\n",
    "\n",
    "By far the most popular nonparametric statistic to compare distribution is the KS test. We compute the empirical cdf $F(x)$ for two samples $\\{x1_i\\} = (1,\\ldots,N_1)$ and $\\{x2_i\\} = (1,\\ldots,N_2)$. *Recall that the empirical cdf can be calculated by sorting the samples and dividing by the sample size.*\n",
    "\n",
    "$$ D = \\mathrm{max}|F_1(x1) - F_2(x2)|$$\n",
    "\n",
    "The core idea is to know how often the value of $D$ computed from our data would arise by chance if both samples were drawn from the same distribution (which is our *null hypothesis* here). \n",
    "\n",
    "Amazingly, ***this does not depend on the underlying distribution we care about!***\n",
    "\n",
    "The probability of obtaining a value of $D$ larger than that observed in our data is\n",
    "\n",
    "$$ Q_\\mathrm{KS}(\\lambda) = 2\\sum_{k=1}^\\infty (-1)^{k-1}\\exp(-2k^2\\lambda^2)$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\\lambda = \\left(0.12 + \\sqrt{n_e} + \\frac{0.11}{\\sqrt{n_e}} \\right)D $$\n",
    "\n",
    "and the ***effective number of data points***, $n_e$, is\n",
    "\n",
    "$$ n_e = \\left( \\frac{1}{N_1} + \\frac{1}{N_2}\\right)^{-1} = \\frac{N_1 N_2}{N_1 + N_2}$$\n",
    "\n",
    "If the probability that $D$ were drawn by chance is very small (low $p$-value) then we can reject the null hypothesis that the two samples were drawn from the same distribution.\n",
    "\n",
    "For large $n_e$ we have $\\lambda\\approx\\sqrt{n_e}D$. In fact, for $n_e>10$ we can bypass $Q_\\mathrm{KS}$ entirely and compute the value of D that would correspond to a given significance level $\\alpha$,\n",
    "\n",
    "$$ D_\\mathrm{KS} = \\frac{C(\\alpha)}{\\sqrt{n_e}}$$\n",
    "\n",
    "where $C(\\alpha) = \\sqrt{-\\frac{1}{2}\\ln(\\alpha/2)}$.\n",
    "\n",
    "##### Guarda gli esempi che ci sono sul suo notebook!\n",
    "\n",
    "---\n",
    "\n",
    "There are many other nonparametric tests, in particular the **Anderson-Darling test** to check whether a sample is consistent with having been drawn from a Gaussian distribution. We don't have time to do a deep dive here, but see the textbook for further details. ( OTTIMALE PER FARE QUEL TIPO DI LAVORO, arguably better than KS test. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonparametric Modeling & Histograms \n",
    "\n",
    "A volte gli istogrammi \"non bastano più\".\n",
    "\n",
    "Imagine you have some one-dimensional (\"univariate\") data that you would like to try to understand.  Where by \"understand\" we mean \"know the distribution in the measured space\", i.e., you want to know the probability distribution function. Our constant friend is the histogram, and it's usually the first thing any of us do on new data. Simple, right? Not quite...\n",
    "\n",
    "Let's work through some examples to see what problems we encounter and how we might overcome them.\n",
    "\n",
    "##### What bins should I pick to get the \"best\" representation of my data?\n",
    "\n",
    "We find that small changes in parameters to the histogram function *significantly* change the PDF.  That's bad, because the underlying data clearly have **not** changed. One of the problems with histograms is that some bins end up with little (or no) data.  We can fix this by making **variable-width bin sizes** that have the ***same number of objects in each bin***.  How can we do this?\n",
    "\n",
    "##### Vedi Notebook\n",
    "\n",
    "Again, this can look pretty different depending on what the number of objects you choose as the minimum for each bin and compared to the plots above.  And it looks a lot different from the plots above.\n",
    "\n",
    "> So, what is the \"right\" way to set the bin size? There is no \"right\" way, but there are some useful rules of thumb.\n",
    "\n",
    "**\"Scott's rule\"** suggests that the optimal bin width is \n",
    "\n",
    "$$\\Delta_b = \\frac{3.5\\sigma}{N^{1/3}}.$$\n",
    "\n",
    "That's great, but what if we don't know the standard deviation, $\\sigma$ (e.g., if the distribution isn't really Gaussian)?  ( devi sapere la sigma, ma se i tuoi dati non sono gaussiani non va tanto bene [ to put it mildly ] ! )\n",
    "\n",
    "We can then instead used the **\"Freedman-Diaconis rule\"**: \n",
    "\n",
    "$$\\Delta_b = \\frac{2(q_{75}-q_{25})}{N^{1/3}} = \\frac{2.7\\sigma_G}{N^{1/3}}.$$  \n",
    "\n",
    "Let's try that. Remember that you can compute $\\sigma_G$ using `astroML`.\n",
    "\n",
    "##### Rug Plot\n",
    "\n",
    "I punti dell'istogramma non sanno bene \"dove\" sono i punti -> plotto individual bars, per far vedere dove stanno.\n",
    "\n",
    "> Il miglior modo per \"fare le cose\" comunque resta il KDE\n",
    "\n",
    "---\n",
    "\n",
    "## Kernel Density Estimation (KDE).\n",
    "\n",
    "\n",
    "This is where **[Kernel Density Estimation](https://en.wikipedia.org/wiki/Kernel_density_estimation) (KDE)** comes in:\n",
    "- In short the idea here is to represent each data point not as a delta function, but rather as a distribution (e.g., a Gaussian).  \n",
    "- Those individual distributions (\"kernels\") are summed up to produce the PDF.  \n",
    "- One of the advantages of this is that it combines the best of \n",
    "    1. the histogram (tells us the relative height of the distribution) \n",
    "    2. the rug plot (centers the data points at the actual location of the data instead of within some arbitrary bin)\n",
    "\n",
    "Just about any distribution can be used as the kernel, but the most common are a **Gaussian kernel** and an **Epanechnikov kernel**.  One downside of the Gaussian kernel is that the tails are technically infinite in extent.  So each point has some finite probability of being *everywhere*.  The Epanechnikov kernel has truncated wings.  \n",
    "\n",
    "###### NB. Lato negativo del Gaussian Kernel: avere infinite tails -> il punto è tipo \"ovunque\"...? Not great.\n",
    "\n",
    "One still has the problem of deciding the width of the kernel (e.g., for the Gaussian the *\"mean\"* is fixed at the value of the point, but how wide should you make the Gaussian?). For now, we'll just play with the widths by hand to see what might work best.  N.B. the widths of the kernel distribution are referred to as **\"bandwidth\"**.\n",
    "\n",
    "> In practice, the bandwidth is almost always the key parameter of a KDE representation, the exact shape of the kerne doesn't really matter too much. ( non c'è una dimostrazione, fidati )\n",
    "\n",
    "Nota che la KDE è un modo molto più potente di valutare dove stanno le cose: ti restituisce una specie di istrogramma \"smooth\", rispetto ad un coso a gradini!\n",
    "\n",
    "**HISTOGRAMS TAKE-AWAY MESSAGE:** \n",
    "\n",
    "Making a histogram is the first-cut we make of data, and it's certainly one of the most sensible things to try to get a feel for the data. But we can't just do it without thinking. We need to explore bin sizes and KDE smoothing bandwidths to tease out the structure in the distributions, and overcome any finite sample effects in bins by potentially having variable bin widths.\n",
    "\n",
    "Finally, the normalized bin height of a histogram can simply be understood as\n",
    "\n",
    "$$ f_k = \\frac{n_k}{\\Delta_b N}$$\n",
    "\n",
    "where $k$ indexes the bin, $n_k$ is the occupancy number of the bin, $\\Delta_b$ is the bin width, and $N$ is the total sample size. If we want to assign **uncertainties** to each bin height (not often done, but its good practice) then we can quote\n",
    "\n",
    "$$ \\sigma_k = \\frac{\\sqrt{n_k}}{\\Delta_b N}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
