{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per prima cosa, guardiamo la correzione dell'esercizio. Guardala sul suo github.\n",
    "\n",
    "Oss. 1: lui \"pulisce\" i dati con `np.isnan` -> T90 distribution analysis\n",
    "\n",
    "Oss. 2: ha aggiustato i dati per avere una distribuzione \"più gaussiana\"\n",
    "\n",
    "Oss. 3: it's easier to confuse lGRB with a sGRB, according to this model\n",
    "\n",
    "Oss. 4: il plot lì sotto con Kmeans ti fa vedere type1 e type2 errors, praticamente\n",
    "\n",
    "Oss. 5: un modelling più sofisticato dovrebbe trovare che la separazione tra i due modelli è a circa 2-3 s\n",
    "\n",
    "Oss. 6: il fatto che il grosso dei punti stia sotto la barra d'errore mi fa capire che i punti NON sono molto lontani dai due cluster centers!\n",
    "\n",
    "Oss. 7: nota che la coda a sinistra è... un po' cannata, probabilmente. Meanwhile, puoi provare a sistemare con MCMC ( ? )\n",
    "\n",
    "Oss. 8: redshift data => selection effects?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional Reduction, part I\n",
    "\n",
    "**Curse of Dimensionality**: pensa a quando devi prendere un pc e metti tutti i filtri su amazon. Quanta probabilità c'è che un pc in magazzino abbia una delle feature filtrate? Ok. Feature indipententi => prodotto delle probabilità . Beh, c'è una marea di roba da filtrare!\n",
    "\n",
    "> The more dimensions my data spans, the more points are needed to uniformly sample the space.\n",
    "\n",
    "Esempio più matematico: rapporto tra area di cerchio vs. area del quadrato? tra volume di sfera e volume di cubo? => trovi $f_{D}$ come nel notebook. Nota che l'errore schizza al 100% perché giustamente stai \"buttando via\" tutti i punti ( la sfera infinito-dimensionale ha volume zero )\n",
    "\n",
    "> It is crucial to try and reduce the dimensionality of my problem.\n",
    "\n",
    "### PCA: Principal Component Analysis\n",
    "\n",
    "Supponiamo di avere i punti distribuiti in quel modo in figura: posso ri-definire i miei dati in modo tale da \"ruotare\" il problema in modo da MINIMIZZARE ..? e MASSIMIZZO la varianza -> trovo componente principale, e quella ortogonale.\n",
    "\n",
    "**Guarda come funziona sul notebook, oggi non ce la posso fare.**\n",
    "\n",
    "##### Preparing Data for PCA\n",
    "\n",
    "1. Sottrai la media -> l'algoritmo lavora con numeri piccoli\n",
    "2. Dividi per la varianza \n",
    "3. NOTEBOOK\n",
    "\n",
    "Vedi codice d'esempio, again.\n",
    "\n",
    "LE COMPONENTI ( $N' \\leq N$ ) VENGONO RESTITUITE IN ORDINE DI EIGENVALUE: PRIMA IL PIU' GRANDE, POI QUELLI PIU' PICCOLI.\n",
    "\n",
    "##### SDSS Sky Survey Data: Spectra\n",
    "\n",
    "Guarda i risultati del codice: 88% dei dati sta nel \"cluster della media\" ( riscrivi bene! ); 6% è un'altra componente, 2% è la terza componente, e così via. IN PRATICA HA TROVATO DELLE COMPONENTI DEGLI SPETTRI, E ALLA FINE TROVA LE INDIVIDUAL LINES.\n",
    "\n",
    "**Scree Plots**: plot the eigenvalue number as a function of the eigenvalue. Vedi grafico: 93% of the information è contenuta in un problema a 10 parametri. Vuoi TUTTE le informazioni? Devi tenere TUTTO. Ok, ma spesso e volentieri non ti serve!\n",
    "\n",
    "Posso anche fare il problema al contrario: supponiamo che voglio tenere il X% dell'informazione. Come si fa? Due opzioni: cicli sullo scree plot finché non trovi il lvalore che verchi, altrimenti usi PCA ma se gli dai *n_components* tra 0 e 1 fa esattamente quel che vuoi!\n",
    "\n",
    "Costruiamo il plot con more and more components: mi avviino sempre di più al risultato \"reale\", anche se ho molte meno dimensioni!\n",
    "\n",
    "Con tot eigenvalues e tot eigenspectra hai compresso il dataset: nel caso in esempio, è l'1% di quel che c'era prima, MA spiega magari il 90%+ dei tuoi dati! <br> $$:°$$\n",
    "\n",
    "##### CAVEAT\n",
    "\n",
    "* Difficoltà ad interpretare le componenti.\n",
    "* PCA IS LINEAR: se i dati non sono lineari, auguri. Serve altro, o un gran numero di componenti.\n",
    "* Impracticality of PCA for **very** large data sets.\n",
    "\n",
    "How many components should I keep? \n",
    "\n",
    "* Fai cross validation\n",
    "* Guarda gli scree plots e guarda quando cambia drasticamente la pendenza\n",
    "\n",
    "---\n",
    "\n",
    "### Missing Data\n",
    "\n",
    "Puoi usare il PCA per ricostruire dove ti mancano dei dati. \n",
    "\n",
    "Fitto il PCA tenendo conto dei \"buchi\" nel mio data set: l'idea è che i dati mancanti NON toccano la principal component! \n",
    "\n",
    "Esempio: profile of pulsars, de-compose in \"eigenpulses\" -> Decomponi fino al noise!\n",
    "\n",
    "### NMF: Non-negative Matrix Factorization\n",
    "\n",
    "Ho parlato della difficoltà nell'interpretare le componenti del mio PCA. \"Quali sono le due matrici, quella di rotazione e quella del cambio di base, che minimizzano (?) lo square error con i miei dati originali?\"\n",
    "\n",
    "### ICA: Independent Component Analysis\n",
    "\n",
    "\"Cocktail Party\" problem: vuoi tipo separare le voci delle singole persone per ascoltarle? Qui è analogo: cerco sequence of projections such that the data are AS FAR from gaussian AS POSSIBLE!\n",
    "\n",
    "Nota che le ICA components disegnate NON tirano fuori la componente più importante, bensì tirano fuori tanti \"disegni\" molto diversi tra loro!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
