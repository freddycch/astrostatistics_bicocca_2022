{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cb39bd",
   "metadata": {},
   "source": [
    "# Regression\n",
    "\n",
    "Regression = determinare la relazione tra una variabile indipendente $x$ e la variabile ad essa dipendente $y$, dove ~il valore di aspettazione di *y*~ è $E[y]=E[y|x]$.\n",
    "\n",
    "Diversamente da density estimation, clustering e dimensional reduction, **la regression è un supervised process**.\n",
    "\n",
    "In generale, vorrei inferire la *vera* pdf che genera le mie osservazioni, da un sample multi-dimensionale di dati che pesco da quella pdf, usando modelli con o senza parametri; *è difficile*, quindi uso la regressione per trovare E[y|x] *al posto della pdf vera e propria*. In altre parole, dato x vorrei la pdf di y, ma mi accontento di sapere un *point estimate*.\n",
    "\n",
    "Banalissimo esempio sul notebook: fittare una retta ad un set di punti . $y = \\theta_{1} x + \\theta_{0}$ -> cosa \"va bene\" nello spazio dei parametri $(\\theta_{0},\\theta_{1})$? Più punti aggiungo, più metto constraints!\n",
    "\n",
    "Nell'istante in cui ho incertezza sul mio punto, le linee che vedevo nel primo disegno diventano delle strisciate di probabilità sul grafico nello spazio dei parametri -> \"è da qualche parte qui\".\n",
    "\n",
    "### Bayesian perspective on regression\n",
    "\n",
    "If we take a Bayesian approach to regression, then we can easily write the posterior pdf for the model parameters as\n",
    "\n",
    "$$p(\\theta|\\{x_i, y_i\\},I) \\propto p(\\{x_i,y_i\\} | \\theta, I) \\, p(\\theta, I),$$\n",
    "\n",
    "where the data likelihood is the product over the individual event likelihoods, which can be written as\n",
    "\n",
    "$$p(y_i|x_i,{\\theta}, I) = e(y_i|y)$$\n",
    "\n",
    "with $e(y_i|y)$ being the probability of getting $y_i$ given the adopted model of $y=f(x|\\theta)$ (i.e. the error distribution). If the error distribution is Gaussian then,\n",
    "\n",
    "$$p(y_i|x_i,{\\theta}, I) = {1 \\over \\sigma_i \\sqrt{2\\pi}} \\, \\exp{\\left({-[y_i-f(x_i|{\\theta})]^2 \\over 2 \\sigma_i^2}\\right)}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f54457",
   "metadata": {},
   "source": [
    "## 2-D Linear Regression <a class=\"anchor\" id=\"two\"></a>\n",
    "\n",
    "Let's start with the simplest case: a linear model with independent variable, $x$, and dependent variable, $y$:\n",
    "\n",
    "$$y_i = \\theta_0 + \\theta_1 x_i + \\epsilon_i,$$\n",
    "\n",
    "where $\\theta_0$ and $\\theta_1$ are the coefficients of the model that we are trying to estimate, and $\\epsilon_i$ is an additive noise term with $\\epsilon_i = \\mathscr{N}(0,\\sigma_i)$. \n",
    "\n",
    "As we've seen many times, the full data likelihood is the product of each individual event likelihood:\n",
    "\n",
    "$$p(\\{y_i\\}|\\{x_i\\},{\\theta}, I) \\propto \\prod_{i=1}^N \\exp \\left(\\frac{-(y_i- (\\theta_0 + \\theta_1x_{i}))^2}{  2\\sigma_i^2}\\right).$$\n",
    "\n",
    "Assuming a flat/uninformative prior, the log of the posterior probability is then\n",
    "\n",
    "$$\\ln(p({\\theta}|\\{x_i, y_i\\},I)) \\propto \\ln \\mathcal(L)  \\propto \\sum_{i=1}^N \\left(\\frac{-(y_i- (\\theta_0 + \\theta_1x_{i}))^2}{  2\\sigma_i^2}\\right).$$\n",
    "\n",
    "***We want to find the values of $\\theta$ that maximize this expression, which is the same as minimizing the least squares.*** \n",
    "\n",
    "### Homoscedastic uncertainty scenario\n",
    "\n",
    "With uncertainties that are the same for all points, this minimization yields\n",
    "\n",
    "$$\\theta_1 = \\frac{\\sum_i^N x_i y_i - \\bar{x}\\bar{y}}{\\sum_i^N(x_i-\\overline{x})^2},$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\theta_0 = \\overline{y} - \\theta_1\\overline{x},$$\n",
    "\n",
    "where $\\overline{x}$ and $\\overline{y}$ are the mean values.\n",
    "\n",
    "The estimate of the variance and the standard errors of the estimated parameters are\n",
    "\n",
    "$$\\sigma^2 = \\sum_{i=1}^N (y_i - \\theta_0 + \\theta_1 x_i)^2,$$\n",
    "\n",
    "$$\\sigma_{\\theta_1}^2 = \\sigma^2\\frac{1}{\\sum_i^N(x_i-\\overline{x})^2},$$\n",
    "\n",
    "$$\\sigma_{\\theta_0}^2 = \\sigma^2\\left(\\frac{1}{N} + \\frac{\\overline{x}^2}{\\sum_i^N(x_i-\\overline{x})^2}\\right).$$\n",
    "\n",
    "### Heteroscedastic uncertainty scenario\n",
    "\n",
    "If the errors are instead different for each point, it is better to think of the problem in matrix notation:\n",
    "\n",
    "$$Y = M \\theta$$\n",
    "\n",
    "where $Y$ is an $N$-dimensional vector of values ${y_i}$,\n",
    "\n",
    "$$Y = \\left[\n",
    "    \\begin{array}{c}\n",
    "    y_0\\\\\n",
    "    \\vdots\\\\\n",
    "    y_{N-1}\n",
    "    \\end{array}\n",
    "    \\right].\n",
    "$$\n",
    "\n",
    "For the straight line model, $\\theta$ is simply a two-dimensional vector of regression coefficients,\n",
    "\n",
    "$$\n",
    "\\theta = \\left[\n",
    "            \\begin{array}{c}\n",
    "            \\theta_0\\\\\n",
    "            \\theta_1\n",
    "            \\end{array}\n",
    "          \\right],\n",
    "$$\n",
    "\n",
    "and **$M$ is a called the design matrix**\n",
    "\n",
    "$$\n",
    "M = \\left[\n",
    "        \\begin{array}{cc}\n",
    "        1 & x_0\\\\\n",
    "        \\vdots & \\vdots\\\\\n",
    "        1 & x_{N-1}\n",
    "        \\end{array}\n",
    "    \\right],\n",
    "$$\n",
    "\n",
    "where the constant in the first column of $M$ captures the zeropoint (i.e. the constant $y$-intercept) in the regression.\n",
    "\n",
    "The log-likelihood can then be written as\n",
    "\n",
    "$$ \\ln\\mathcal(L) = -\\frac{1}{2}(Y-M\\theta)^T C^{-1} (Y-M\\theta) - \\frac{1}{2}\\ln[\\mathrm{det}(C)]$$\n",
    "\n",
    "where we encapsulate uncertainties in the ($N\\times N$) covariance matrix\n",
    "\n",
    "$$C=\\left[\n",
    "        \\begin{array}{cccc}\n",
    "        \\sigma_{0}^2 & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "        0 & 0 & \\cdots & \\sigma_{N-1}^2 \\\\\n",
    "        \\end{array}\n",
    "    \\right].\n",
    "$$\n",
    "\n",
    "and the maximum likelihood solution for the regression can be analytically solved and expressed as\n",
    "\n",
    "$$\\theta = (M^T C^{-1} M)^{-1} (M^T C^{-1} Y),$$\n",
    "\n",
    "which minimizes the sum of squares, and gives uncertainties on $\\theta$ of \n",
    "\n",
    "$$\\Sigma_\\theta = \\left[\n",
    "                    \\begin{array}{cc}\n",
    "                    \\sigma_{\\theta_0}^2 & \\sigma_{\\theta_0\\theta_1} \\\\\n",
    "                    \\sigma_{\\theta_0\\theta_1} & \\sigma_{\\theta_1}^2\n",
    "                    \\end{array}\n",
    "                  \\right]\n",
    "                    = [M^T C^{-1} M]^{-1}.\n",
    "$$\n",
    "\n",
    "With `numpy` it is straightforward to write the matrices as multi-dimensional arrays, and do the linear algebra (provided the matrices are invertible), then calculate the regression coefficients.\n",
    "\n",
    "Il codice è nel suo notebook.\n",
    "\n",
    "Much easier with [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) from `Scikit-Learn`.\n",
    "\n",
    "Sempre lì, nel notebook.\n",
    "\n",
    "So, what we did was to replace\n",
    "```\n",
    "C = np.identity(len(X))*(dy*dy)\n",
    "M = np.column_stack((np.ones(len(X)),X))\n",
    "A = np.dot(np.dot(M.transpose(),np.linalg.pinv(C)),M)\n",
    "B = np.dot(np.dot(M.transpose(),np.linalg.pinv(C)),y)\n",
    "theta = np.dot(np.linalg.pinv(A),B)\n",
    "\n",
    "y_pred = np.dot(M_new,theta) # using design matrix at new x locations\n",
    "```\n",
    "\n",
    "with\n",
    "```\n",
    "LRmodel = LinearRegression()\n",
    "LRmodel.fit(X, y, sample_weight=noise_std)\n",
    "y_pred = LRmodel.predict(X_new)\n",
    "```\n",
    "\n",
    "A reminder on nomenclature for `Scikit-Learn`: \n",
    "- $X$ is the multidimensional matrix of $N$ \"objects\", each with $K$ attributes.  \n",
    "- $y$ is the dependent variable that represents the measured value for each of those $N$ objects.  \n",
    "- What is new here is that are adding $dy$, which is the uncertainty on $y$. \n",
    "\n",
    "The fitting algorithms are going to be of the form:\n",
    "\n",
    "```model.fit(X,y,dy)```.\n",
    "\n",
    "## Multivariate linear regression <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "In the above cases, we were doing 2-D linear regression with a univariate $X$.  If $X$ is instead multivariate, then we fit a hyperplane rather than a straight line\n",
    "\n",
    "$$y_i =\\theta_0 + \\theta_1x_{i1} + \\theta_2x_{i2} + \\cdots +\\theta_kx_{ik} + \\epsilon_i.$$\n",
    " \n",
    "The design matrix, $M$, is now \n",
    "\n",
    "$$M = \\left(\n",
    "        \\begin{array}{ccccccc}\n",
    "        1 & x_{01} & x_{02} & . & x_{0k}\\\\\n",
    "        1 & x_{11} & x_{12} & . & x_{1k}\\\\\n",
    "        . & . & . & .  & . \\\\\n",
    "        1 & x_{N1} & x_{N2} & . & x_{Nk}\\\\\n",
    "        \\end{array}\n",
    "      \\right)\n",
    "$$\n",
    "\n",
    "but the whole formalism is exactly the same as before.\n",
    "\n",
    "Note that Scikit-Learn's [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) routine does not explicitly account for heteroscedastic error. This is unfortunate, but this generalization is implemented in [astroML.linear_model.LinearRegression](https://www.astroml.org/modules/generated/astroML.linear_model.LinearRegression.html).\n",
    "\n",
    "## Polynomial Regression <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "We introduced regression with examples of straight-line fitting, but we can think of it more generically in terms of **[polynomical regression](https://en.wikipedia.org/wiki/Polynomial_regression)** with $y=f(x|\\theta)$ and\n",
    "\n",
    "$$y_i =\\theta_0 + \\theta_1 x_{i} + \\theta_2 x_{i}^2 + \\theta_3 x_{i}^3 + \\cdots.$$   \n",
    "\n",
    "For example, maybe $y$ depends on the area or volume of an object, but you have mesured the length. More generally, this is like fitting a model that is a Taylor expansion of the exact $f(x)$ to determine amplitudes of the different polynomial terms.\n",
    "\n",
    "For polynomical regression, the design matrix $M$ is now\n",
    " \n",
    "$$\n",
    "M = \\left(\n",
    "        \\begin{array}{cccccc}\n",
    "        1 & x_{0} & x_{0}^2 & x_{0}^3\\\\\n",
    "        1 & x_{1} & x_{1}^2 & x_{1}^3\\\\\n",
    "        . & . & . & . \\\\\n",
    "        1 & x_{N} & x_{N}^2 & x_{N}^3\\\\\n",
    "        \\end{array}\n",
    "    \\right).\n",
    "$$\n",
    "\n",
    "**NOTE:** Be careful with terminology!! ***This is still Linear Regression since the model only depends linearly on the parameters.*** The \"linear\" in linear regression corresponds to these parameters, not the independent variable $x$.\n",
    "\n",
    "As with linear regression, we'll use `PolynomialRegression` from `AstroML`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219c200f",
   "metadata": {},
   "source": [
    "## Basis function regression <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "If we consider a function in terms of a sum over bases (this can be polynomials, Gaussians, sines) then we can solve for the linear amplitude coefficients using regression. ***So regression with arbitrary basis terms that may be non-linear in $x$ is still linear regression, since the model is linear in the regression parameters.*** \n",
    "\n",
    "Above we have used polynomials, but we could substitute $x_{0}^2$ etc for Gaussians (where we fix $\\sigma$ and $\\mu$ and fit for the amplitude) as long as the attribute we are fitting for is linear. So if straight-line regression is just a special case of polynomial regression, then polynomial regression is just a special case of basis function regression. All of these are linear regression techniques.\n",
    "\n",
    "## Kernel Regression <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "In the case of Gaussian Basis Regression, if you think about it, we were back to the old problem of making a histogram.  Specifically, our Gaussians were evenly spaced over the range of interest. If we instead placed Gaussians at the location of every data point, we get Gaussian Kernel Regression instead. Or just **[Kernel Regression](https://en.wikipedia.org/wiki/Kernel_regression)** more generally since we don't *have* to have a Gaussian kernel function. It is also called **Nadaraya-Watson regression**.\n",
    "\n",
    "Given a kernel $K(x_i,x)$ (e.g., a Gaussian or top-hat) at each point we estimate the function value by\n",
    "\n",
    "$$f(x|K) = \\frac{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right) y_i}\n",
    "{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right)},$$\n",
    "\n",
    "which is a weighted sum of $y$ (weighted by distance) with\n",
    "\n",
    "$$w_i(x) = \\frac{ K\\left( \\frac{||x_i-x||}{h} \\right)}\n",
    "{\\sum_{i=1}^N K\\left( \\frac{||x_i-x||}{h} \\right)}.$$\n",
    "\n",
    "This **locally weighted regression** technique drives the regressed value to the nearest neighbor (when we have few points) which helps with extrapolation issues. As we saw with KDE, defining the correct bandwidth of the kernel is more important than the shape of the kernel itself and is done through **cross-validation**.\n",
    "\n",
    "Nadaraya-Watson is implemented in `AstroML` as follows (note that the y errors are irrelevant, at least in this implementation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0105ec2",
   "metadata": {},
   "source": [
    "## Over/Under-fitting <a class=\"anchor\" id=\"seven\"></a>\n",
    "\n",
    "We already talked a little bit about overfitting, but let's dive down deeper now that we are trying to fit complicated models. We'll use a 1-D model with homoscedastic errors for the sake of illustration, but this discussion applies to more complicated data as well.\n",
    "\n",
    "To be clear, our data consists of $X_{\\rm train}$, $y_{\\rm train}$, and $X_{\\rm test}$ and we are trying to predict $y_{\\rm test}$.  \n",
    "\n",
    "Let's take an example where\n",
    "\n",
    "$$0\\le x_i \\le 3$$\n",
    "\n",
    "and\n",
    "\n",
    "$$y_i = x_i \\sin(x_i) + \\epsilon_i,$$\n",
    "\n",
    "where the noise, $\\epsilon_i$ is given by $\\mathscr{N}(0,0.1)$.\n",
    "\n",
    "Vedi esempio sul notebook: 20 evenly spaced points from this distribution + fit with a straight line.\n",
    "\n",
    "- This model **underfits** the data and is said to be \"biased\" (in the sense that the estimated model parameters deviate significantly from the true model parameters).  \n",
    "- A straight line is a polynomial of order 1, so let's try polynomials of higher order.\n",
    "\n",
    "Ok, ma come scelgo la \"risposta giusta\"? La risposta ovviamente è la CROSS VALIDATION.\n",
    "\n",
    "## Cross-validation <a class=\"anchor\" id=\"eight\"></a>\n",
    "\n",
    "Recap. In case you're still not sure about $k$-fold CV, the procedure is the followig:\n",
    "\n",
    "- Split the data into $k+1$ subsets; the test set and $k$ CV sets. How you do this is up to you, but typically through random shufflings with equal numbers of points.\n",
    "- $k$ models are trained, each time leaving out one of the CV sets in order to measure the CV error.\n",
    "- The final training and CV error can be computed using the mean or median of the set of results. The median can be more reliable.\n",
    "\n",
    "See the following GIF of $3$-fold CV from the [Wikipedia article](https://www.wikiwand.com/en/Cross-validation_(statistics)).\n",
    "\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/4/4b/KfoldCV.gif/1920px-KfoldCV.gif?1616979144969)\n",
    "\n",
    "More regression coefficients improve the ability of the model to fit all the points (reduced **bias**), but at the expense of model complexity and **variance**. *Of course* we can fit a Nth-degree polynomial to N data points, but that would be foolish. We'll determine the best trade-off between bias and variance through [cross-validation](https://en.wikipedia.org/wiki/Cross-validation).\n",
    "\n",
    "When we increase the complexity of a model, the data points fit the model more and more closely. However, this process does not necessarily result in a better fit to the data. Rather, if the degree is too high, then we are ***overfitting*** the data. The model has high variance, meaning that a small change in a training point can change the model dramatically.  \n",
    "\n",
    "We can evaluate this using a **training set** ($50-70\\%$ of sample), a **cross-validation set** ($15-25\\%$) and a **test set** ($15-25\\%$).\n",
    "\n",
    "The training set is used the determine the model paramters, $\\theta_j$.  The training data and cross-validation data then are both used to evaluate the training and cross-validation rms errors ($\\epsilon_{\\rm tr}$ and $\\epsilon_{\\rm CV}$; evaluated here for polynomial regression):\n",
    "\n",
    "$$\\epsilon_{\\rm cv/tr} = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{N_{\\rm cv/tr}}\n",
    "  \\left[y_i - \\sum_{m=0}^d \\theta_0^{(n)}x_i^m\\right]^2}$$\n",
    "\n",
    "> *Why do we need both a training set and a cross-validation set?* \n",
    "> - The **model parameters, $\\theta_j$, are learned from the training set**,\n",
    "> - But the **\"hyperparameters\" (in this case the model degree) are learned from the cross-validation set**. \n",
    "\n",
    "> *The test set then provides the best estimate of the error expected for a new set of unlabeled data.*\n",
    "\n",
    "We show this graphically in the next figure (Ivezic, 8.14), where the **training and cross-validation rms errors are computed as a function of polynomial degree**, and also compared with the **model BIC**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b7cf4",
   "metadata": {},
   "source": [
    "- For low order, both the training and CV error are high. This is sign of a **high-bias model that is underfitting** the data.  \n",
    "- For high order, the training error becomes small (by definition), but the CV error is large. This is the sign of a **high-variance model that is overfitting** the data. It is matching the subtle fluctuations in the training data that aren't really real, and this shows up in the CV analysis.\n",
    "- The BICs give similar results.\n",
    "\n",
    "Hopefully that helps you understand how to use cross validation to help you both **fit your model and decide on the optimal level of model complexity**, but maybe it doesn't help you apply it to your own data.  \n",
    "\n",
    "We've already seen how to do this with **[GridSearchCV](https://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV)**, where for this case, we'd just be varying one parameter (even though `GridSearchCV` can vary many). That is, if the algorithm you're using support's the scikit-learn GridSearchCV infrastructure..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be662b",
   "metadata": {},
   "source": [
    "## Do we need more data? Learning Curves\n",
    "\n",
    "Of course more data means a better fit... but a some point data are all the same!\n",
    "\n",
    "We can use a tool called a **[learning curve](https://en.wikipedia.org/wiki/Learning_curve)** to determine if (for a given model) having more training data would help improve the model fitting. This is a different question than above-- rather than try to get a better model of the data, we're trying to improve the quality of our data set. \n",
    "\n",
    "The training and CV error are computed as a function of the number of training points. In general:\n",
    "- ***The training error increases with $N_\\mathrm{train}$.*** For a given model, it's easier to fit fewer data points.\n",
    "- ***The CV error decreases wtih $N_\\mathrm{train}$.*** For a given model, a greater number of training points reduces the chances of over-fitting, resulting in better performance of the model in the cross-validation stage.\n",
    "\n",
    "Let's look at this for the same data and model as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c94ee6",
   "metadata": {},
   "source": [
    "We can see two regimes:\n",
    "\n",
    "1. ***The training and CV errors have converged.*** This indicates that the model is dominated by bias. Increasing the number of training points is futile. If the error is too high, you instead need a more complex model, not more training data.\n",
    "2. ***The training error is smaller than the CV error.*** This indicates that the model is dominated by variance.  Increasing the number of training points may help to improve the model.\n",
    "\n",
    "In both cases, for small numbers of training points, the difference between the training and CV errors indicates that more data well help.  \n",
    "\n",
    "For the top plot, the convergence of the training and CV errors indicates that adding further data will not reduce the error as it is dominated by ***bias***. A more sophisticated model is needed. The training error starts at zero because we are predicting the training data perfectly, but the CV error is high, indicating that the training data aren't sufficiently representative.\n",
    "\n",
    "Again, hopefully that helps you understand what learning curves tell you, but maybe it doesn't help you apply it to your own data.  So, let's see how sklearn does it using [sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve).  \n",
    "\n",
    "Below we apply this to the [Boston Housing data](http://scikit-learn.org/stable/datasets/index.html#boston-house-prices-dataset). This data set contains 12 attributes that can be used to predict the price of houses in Boston. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ac9573",
   "metadata": {},
   "source": [
    "So, we barely have enough data.  This is important to know when we are deciding on the number of cross validations.  \n",
    "\n",
    "Once you are happy with the size of your training set and your choice of model, and now just want to get the best model parameters using *all* of the data (not just whatever fraction is in your training set), then we can use **[cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html)**.\n",
    "\n",
    "\"Each sample belongs to exactly one test set, and its prediction is computed with an estimator fitted on the corresponding training set.\"\n",
    "\n",
    "Let's use this to predict the price of houses in Boston where each house gets to be part of one of the test sets by doing a $10$-fold cross validation. We will then plot the *prediction* versus the *actual* value of $y$, which should lie along the line $y=y_\\mathrm{pred}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba2718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
