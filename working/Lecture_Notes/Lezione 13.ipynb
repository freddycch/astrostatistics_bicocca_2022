{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Fin qui ho visto come funziona la divisione dei dati in *training set* e *test set*: brutalmente, serve a non dire \"ehi, tutto bellissimo, fitto perfettamente i dati\" -> cross-validation.\n",
    "\n",
    "Il clustering è PURAMENTE data-driven: non ci sono labels nei tuoi dati, e ancora di più vuoi verificare se il tuo machine learning sia andato a buon fine. A questo punto, entra in gioco anche il *validation set*: è un sottoinsieme del *training set*.\n",
    "\n",
    "* Training set: fitto i dati\n",
    "* Validation set: ottimizzo i parametri del fit\n",
    "* Test set: verifico che la performance sia buona/giusta\n",
    "\n",
    "PROBLEMA: chi è chi? Quali dati vanno in quale set? E soprattutto: così facendo, mi rimangono MENO DATI per fare il machine learning vero e proprio!\n",
    "\n",
    "Nel caso di KDE voglio ho bisogno di fare \"solo\" training e validation. Come decido i sample?\n",
    "\n",
    "1. Divido i dati \"a caso\", e li separo in training set e validation set. Ripetere tot volte, gg.\n",
    "2. K-fold cross-validation: \"rigiro\" i dati in modo tale che ogni data point finisce in un test (VALIDATION?) set once. (?)\n",
    "3. Leave-one-out cross-validation: come il punto 2, ma con K = N dei dati. Ogni volta \"lascio fuori\" un punto.\n",
    "\n",
    "NOTA CHE AUMENTA IL COSTO COMPUTAZIONALE TREMENDAMENTE.\n",
    "\n",
    "Esempio col KDE nel notebook.\n",
    "\n",
    "**Clustering algorithm**: algoritmo che tenta di raggruppare oggetti simili in un data set. E' unsupervised process.\n",
    "\n",
    "##### K-means Clustering\n",
    "\n",
    "$K$-means seeks to minimize the following\n",
    "\n",
    "$$\\sum_{k=1}^{K}\\sum_{i\\in C_k}||x_i - \\mu_k||^2$$\n",
    "\n",
    "where $\\mu_k = \\frac{1}{N_k}\\sum_{i\\in C_k} x_i$\n",
    "\n",
    "In words, this says to\n",
    "  * Take every object in class $C_k$ (as determined by which centroid it is closest to, specifically $C_k(x_i) = \\arg \\min_k ||x_i-\\mu_k||)$\n",
    "  * Compute the mean of the objects in that class\n",
    "  * Subtract that mean from each member of that class and square the norm\n",
    "  * Do that for each class and sum\n",
    "  * Shift the centroids of the *pre-determined* number of classes until this sum is minimized\n",
    "  * Do this multiple times with different starting centroids and take the result with the minimum sum  \n",
    "  \n",
    "GUARDA GLI ESEMPI SUL SUO NOTEBOOK :D\n",
    "\n",
    "##### Mean-Shift Clustering\n",
    "\n",
    "**Mean-shift clustering** works by finding the modes in a kernel density estimator of the distribution. Clustering is achieved by the ***mean-shift algorithm***:\n",
    "\n",
    "1. The KDE of the dataset is computed.\n",
    "2. This allows the gradient of the distribution to be calculated. Easy to do since it's a bunch of overlapping Gaussians.\n",
    "3. Each data point is shifted in the direction of increasing gradient, which drives the points toward the modes. \n",
    "4. This process is iterated until all points have converged with clusters of other points at each of several distinct modes.\n",
    "5. Each data point is then associated with a cluster of other points.\n",
    "\n",
    "Esempio sul suo notebook, con gli stessi dataset di sopra.\n",
    "\n",
    "Mean-shift will not only estimate cluster centers and boundaries, but also the number of clusters!\n",
    "\n",
    "## Correlation functions <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "> ***Correlation functions*** *tell us how far (and on what scales) a distribution of data samples differs from a random distribution.*\n",
    "\n",
    "They have been used extensively in astrophysics, e.g., \n",
    "- examining fluctuations and structure on varying scales of the galaxy density distribution in terms of luminosity, galaxy type, age of the Universe.\n",
    "- examining the two-point correlation function of temperature fluctuations of the cosmic microwave background to unveil the composition of the Universe.\n",
    "- searching for long-timescale correlations in time-series data to examine noise in AGN lightcurves or find GW signals in pulsar-timing data.\n",
    "\n",
    "\n",
    "![](https://www.astroml.org/_images/fig_corr_diagram_1.png)\n",
    "\n",
    "\n",
    "One of the most prominent is the **two-point correlation function** which characterizes the excess probability of finding pairs of points at varying separations when compared to a random distribution. It can be described in terms of the power spectrum of fluctuations, $P(k)$ where $k=2\\pi/\\lambda$ and $\\lambda$ is the scale/wavelength of the fluctuation:\n",
    "\n",
    "$$ \\xi(r) = \\frac{1}{2\\pi^2}\\int dk\\, k^2 P(k)\\frac{\\sin(kr)}{kr}.$$\n",
    "\n",
    "This correlation function can be used to describe the density fluctuations of sources by\n",
    "\n",
    "$$ \\xi(r) = \\left\\langle \\frac{\\delta\\rho(x)}{\\rho}\\frac{\\delta\\rho(x+r)}{\\rho}\\right\\rangle,$$\n",
    "\n",
    "where $\\delta\\rho(x)/\\rho = (\\rho-\\bar\\rho)/\\rho$ is the density contrast relative to the mean $\\bar\\rho$ at position $x$.\n",
    "\n",
    "In many situations in astronomy or cosmology, the spatial correlation function or angular correlation function is modeled as a power-law, e.g. $w(\\theta) = (\\theta/\\theta_0)^\\delta$ (this is astronomy after all, where power-laws rule supreme). Angular correlation functions are often used because we care about *projected structure* on different scales, rather than depth clustering, e.g., in the CMB this kind of angular clustering is indicative of fluctuations in the primordial density field that can unveil the composition of the Universe at the time of last scattering.\n",
    "\n",
    "Higher $n$-point correlation functions can be computed (see the image above), but the two-point function is the most common. Why? When dealing with large populations of sources spread across the Universe, we can often invoke the central limit theorem to describe the statistical distribution of sources-- hence **correlations can be approximated as obeying Gaussian statistics, which are entirely defined by the mean and two-point correlators (i.e. the variance)**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
